{inspect} = import 'util'

{init_tokenizer, end_token, get_next_token, get_text} = import './tokenizer'
{init_symbols, ignorable, next_lbp, led, nud} = import './symbols'
{token_error} = import './errors'


curr_is = fn {curr_token}, expected:
  curr_token.value == expected


curr_value = fn {curr_token}:
  curr_token.value


curr_loc = fn {curr_token}:
  curr_token.loc


next_is = fn {next_token}, expected:
  next_token.value == expected


next_matches = fn {next_token}, regex:
  # TODO string check, could be a symbol
  (
    next_token.value.constructor == String
    && next_token.value.match(regex)
    && true
  )


next_is_end = fn ctx, expected_end:
  next_is(ctx, end_token) || next_is(ctx, expected_end)


next_loc = fn {next_token}:
  next_token.loc


assert_token = fn token, expected, ctx:
  match true:
    token.value != expected:
      throw token_error(
        `Expected ${inspect(expected)} but found ${inspect(token.value)}.`,
        token, ctx
      )


assert_next = fn ctx, expected:
  {next_token} = ctx
  assert_token(next_token, expected, ctx)


assert_curr = fn ctx, expected:
  {curr_token} = ctx
  assert_token(curr_token, expected, ctx)


assert_not_end = fn ctx:
  match true:
    next_is_end(ctx):
      throw token_error(`Unexpected end of code.`, ctx.next_token, ctx)


advance = fn ctx:
  curr_token = ctx.next_token

  [next_token, tokenizer] = pipe [null, ctx.tokenizer]:
    unfold [, tokenizer]:
      # TODO: should it return [tokenizer_ctx, next_token]?
      [next_token, next_tokenizer] = get_next_token(tokenizer)
      match true:
        ignorable(ctx, next_token):
          [null, next_tokenizer]
        else:
          [next_token, next_tokenizer]

    find [token]:
      token != null

  {...ctx, tokenizer, curr_token, next_token}


assert_advance = fn ctx, expected:
  assert_next(ctx, expected)
  advance(ctx)


collect_text = fn ctx, stop_at:
  {curr_token} = ctx

  [text, next_tokenizer] = get_text:: ctx.tokenizer, curr_token.loc.end, stop_at
  [next_token, tokenizer] = get_next_token:: next_tokenizer

  next_ctx = {...ctx, tokenizer, curr_token, next_token}

  assert_not_end(next_ctx)

  [text, advance(next_ctx)]


expression = fn ctx, rbp:
  assert_not_end(ctx)

  [left, next_ctx] = pipe ctx:
    advance
    nud
    unfold [left, ctx]:
      match true:
        rbp < next_lbp(ctx, left):
          led_ctx = advance(ctx)
          [next_left, next_ctx] = led(led_ctx, left)
          [next_left, next_ctx, false]
        else:
          [left, ctx, true]

    find [,,found]:
      found

  [left, next_ctx]


init_parser = fn {code, filename}:
  {
    curr_token: null,
    next_token: {
      loc: {
        start: {pos: 0, line: 1, column: 0},
        end: {pos: 0, line: 1, column: 0}
      }
    },

    tokenizer: init_tokenizer(code, filename),
    symbols: init_symbols()
  }


start_parser = fn ctx: advance(ctx)

