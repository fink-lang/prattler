{inspect} = import 'util'

{init_tokenizer, end_token, get_next_token, get_text} = import './tokenizer'
{init_symbols, ignorable, next_lbp, led, nud} = import './symbols'
{token_error} = import './errors'



curr_value = fn {curr_token}:
  curr_token.value


curr_loc = fn {curr_token}:
  curr_token.loc


curr_is = fn ctx, expected:
  expected == curr_value:: ctx


next_value = fn {next_token}:
  next_token.value


next_loc = fn {next_token}:
  next_token.loc


next_is = fn ctx, expected:
  expected == next_value:: ctx


next_matches = fn ctx, regex:
  value = next_value:: ctx
  # TODO string check, could be a symbol
  (
    value.constructor == String
    && value.match(regex)
    && true
  )


next_is_end = fn ctx, expected_end:
  next_is(ctx, end_token) || next_is(ctx, expected_end)


assert_token = fn token, expected, ctx:
  {value} = token
  match value:
    # TODO: don't use Array methods
    !expected.includes:: ?:
      throw token_error::
        `Expected one of ${inspect(expected)} but found ${inspect(value)}.`
        token
        ctx


assert_next = fn ctx, ...expected:
  {next_token} = ctx
  assert_token:: next_token, expected, ctx


assert_curr = fn ctx, ...expected:
  {curr_token} = ctx
  assert_token:: curr_token, expected, ctx


assert_not_end = fn ctx:
  match ctx:
    next_is_end:: ?:
      throw token_error::
        `Unexpected end of code.`
        ctx.next_token
        ctx


advance = fn ctx:
  curr_token = ctx.next_token

  [next_token, tokenizer] = pipe [null, ctx.tokenizer]:
    unfold [, tokenizer]:
      # TODO: should it return [tokenizer_ctx, next_token]?
      [next_token, next_tokenizer] = get_next_token:: tokenizer
      match true:
        ignorable:: ctx, next_token:
          [null, next_tokenizer]
        else:
          [next_token, next_tokenizer]

    find [token]:
      token != null

  {...ctx, tokenizer, curr_token, next_token}


assert_advance = fn ctx, ...expected:
  assert_next:: ctx, ...expected
  advance:: ctx


collect_text = fn ctx, stop_at:
  {curr_token} = ctx

  [text, next_tokenizer] = get_text:: ctx.tokenizer, curr_token.loc.end, stop_at
  [next_token, tokenizer] = get_next_token:: next_tokenizer

  next_ctx = {...ctx, tokenizer, curr_token, next_token}

  assert_not_end:: next_ctx

  [text, advance:: next_ctx]


expression = fn ctx, rbp:
  assert_not_end:: ctx

  [left, next_ctx] = pipe ctx:
    advance
    nud
    unfold [left, ctx]:
      match true:
        rbp < next_lbp:: ctx, left:
          led_ctx = advance:: ctx
          [next_left, next_ctx] = led:: led_ctx, left
          [next_left, next_ctx, false]
        else:
          [left, ctx, true]

    find [,,found]:
      found

  [left, next_ctx]


init_parser = fn {code, filename}:
  tokenizer = init_tokenizer:: code, filename
  symbols = init_symbols()

  {
    curr_token: null,
    next_token: {
      loc: {
        start: {pos: 0, line: 1, column: 0},
        end: {pos: 0, line: 1, column: 0}
      }
    },

    tokenizer,
    symbols
  }


start_parser = fn ctx:
  advance:: ctx

