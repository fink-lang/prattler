{inspect} = import 'util'

{init_tokenizer, end_token, get_next_token} = import './tokenizer'
{init_symbols, ignorable, next_lbp, led, nud} = import './symbols'
{token_error} = import './errors'
{find} = import './iter'


curr_is = fn {curr_token}, expected:
  curr_token.value == expected


curr_value = fn {curr_token}:
  curr_token.value


curr_loc = fn {curr_token}:
  curr_token.loc


next_is = fn {next_token}, expected:
  next_token.value == expected


next_matches = fn {next_token}, regex:
  # TODO string check, could be a symbol
  next_token.value.constructor == String && next_token.value.match(regex)


next_is_end = fn ctx, expected_end:
  next_is(ctx, end_token) || next_is(ctx, expected_end)


next_loc = fn {next_token}:
  next_token.loc


assert_token = fn token, expected, ctx:
  match true:
    token.value != expected:
      throw:: token_error(
        `Expected ${inspect(expected)} but found ${inspect(token.value)}:`,
        token, ctx
      )
      # TODO
      null


assert_next = fn ctx, expected:
  {next_token} = ctx
  assert_token(next_token, expected, ctx)


assert_curr = fn ctx, expected:
  {curr_token} = ctx
  assert_token(curr_token, expected, ctx)


assert_not_end = fn ctx:
  match true:
    next_is_end(ctx):
      throw:: token_error(`Unexpected end of code:`, ctx.curr_token, ctx)
      # TODO
      null


advance = fn ctx:
  curr_token = ctx.next_token

  [next_token, ignored_tokens, tokenizer] = pipe [null, [], ctx.tokenizer]:
    unfold [, ignored, tokenizer]:
      # TODO: should it return [tokenizer_ctx, next_token]?
      [next_token, next_tokenizer] = get_next_token(tokenizer)
      match true:
        ignorable(ctx, next_token):
          [null, [...ignored, next_token], next_tokenizer]
        else:
          [next_token, ignored, next_tokenizer]

    find:: fn [token]:
      token != null

  {...ctx, tokenizer, curr_token, next_token, ignored_tokens}


assert_advance = fn ctx, expected:
  assert_next(ctx, expected)
  advance(ctx)


ignored_text = fn ctx:
  ctx.ignored_tokens.map(fn {value}: value).join('')


collect_text = fn ctx, ...stop_token_values:
  stop_at = [...stop_token_values]
  {start, end: curr_end} = curr_loc(ctx)

  inital = [
    ctx.curr_token, ctx.next_token,
    ignored_text(ctx), ctx.tokenizer,
    curr_end
  ]

  [curr_token, next_token, text, tokenizer, end] = pipe inital:
    unfold [curr_token, token, text, tokenizer, end]:
      match true:
        !(token.value == end_token || stop_at.includes(token.value)):
          next_text = text + token.value
          [next_token, next_tokenizer] = get_next_token(tokenizer)
          {end: next_end} = next_token.loc

          [token, next_token, next_text, next_tokenizer, next_end, false]
        else:
          [curr_token, token, text, tokenizer, end, true]

    find:: fn [,,,,,found]: found

  next_ctx = {...ctx, tokenizer, curr_token, next_token, ignored_tokens: []}
  assert_not_end(next_ctx)

  [{text, loc: {start, end}}, advance(next_ctx)]


expression = fn ctx, rbp:
  assert_not_end(ctx)

  [left, next_ctx] = pipe ctx:
    advance
    nud
    unfold [left, ctx]:
      match true:
        rbp < next_lbp(ctx, left):
          led_ctx = advance(ctx)
          [next_left, next_ctx] = led(led_ctx, left)
          [next_left, next_ctx, false]
        else:
          [left, ctx, true]

    find:: fn [,,found]: found

  [left, next_ctx]


init_parser = fn {code}:
  {
    curr_token: null,
    next_token: {
      loc: {
        start: {pos: 0, line: 1, column: 0},
        end: {pos: 0, line: 1, column: 0}
      }
    },
    ignored_tokens: [],

    tokenizer: init_tokenizer(code),
    symbols: init_symbols()
  }


start_parser = fn ctx: advance(ctx)

