{inspect} = import 'util'

{matches} = import '@fink/std-lib/regex.fnk'
{is_empty} = import '@fink/std-lib/iter.fnk'

{init_tokenizer, get_next_token, get_text} = import './tokenizer.fnk'
{init_symbols, ignorable, next_lbp, led, nud} = import './symbols.fnk'
{add_error} = import './errors.fnk'


# TODO: should it live here
ingnorable_token = {ignorable: true}


start_token = dict:
  start_token: true
  loc: dict:
    start: {pos: 0, line: 1, column: 0}
    end: {pos: 0, line: 1, column: 0}


has_errors = fn ctx:
  match ctx:
    {errors: not is_empty ?}: true
    else: false


curr_value = fn {curr_token}:
  curr_token.value


curr_loc = fn {curr_token}:
  curr_token.loc


curr_is = fn ctx, expected:
  expected == curr_value ctx


next_value = fn {next_token}:
  next_token.value


next_loc = fn {next_token}:
  next_token.loc


next_is = fn ctx, expected:
  expected == next_value ctx


next_is_any = fn ctx, ...expected:
  # TODO: use std-lib or in operator
  expected.includes next_value ctx


next_matches = fn ctx, regex:
  {next_token} = ctx
  match next_token:
    {end: true}:
      false
    else:
      matches next_token.value, regex



next_is_end = fn ctx, expected_end:
  match ctx:
    {next_token: {end: true}}: true
    next_is ?, expected_end: true
    else: false


advance = fn ctx:
  curr_token = ctx.next_token

  [next_token, tokenizer] = pipe ctx.tokenizer:
    unfold tokenizer:
      # TODO: should it return [tokenizer_ctx, next_token]?
      [next_token, next_tokenizer] = get_next_token tokenizer

      token = match next_token:
        ignorable ctx, ?: ingnorable_token
        else: next_token

      ([token, next_tokenizer], next_tokenizer)

    find [token]:
      token != ingnorable_token

  {...ctx, tokenizer, curr_token, next_token}


advance_expected = fn ctx, ...expected:
  match ctx:
    next_is_any ?, ...expected:
      advance ctx
    else:
      {value} = ctx.next_token
      [, next_ctx] = add_error ctx,
        'Expected one of ${inspect expected} but found ${inspect value}.'
        ctx.next_token
      next_ctx


collect_text = fn ctx, stop_at:
  {curr_token} = ctx

  [text, next_tokenizer] = get_text ctx.tokenizer, curr_token.loc.end, stop_at
  [next_token, tokenizer] = get_next_token next_tokenizer

  next_ctx = {...ctx, tokenizer, curr_token: text, next_token}

  match next_ctx:
    next_is_end ?:
      [, end_ctx] = add_error next_ctx,
        'Unexpected end of code.'
        next_ctx.next_token
      # TODO: advance end_ctx
      [text, end_ctx]
    else:
      [text, advance next_ctx]


expression = fn ctx, rbp:
  match ctx:
    next_is_end ?:
      add_error ctx, 'Unexpected end of code.', ctx.next_token

    else:
      [left, next_ctx] = pipe ctx:
        advance
        nud
        unfold [left, ctx]:
          match ctx:
            has_errors ?:
              [left, ctx, true]

            rbp < next_lbp ?, left:
              led_ctx = advance ctx
              [next_left, next_ctx] = led led_ctx, left
              [next_left, next_ctx, false]

            else:
              [left, ctx, true]

        find [,,found]:
          found

      [left, next_ctx]


init_parser = fn {code, filename}:
  tokenizer = init_tokenizer code, filename
  symbols = init_symbols _

  dict:
    curr_token: start_token
    next_token: start_token
    errors: []
    tokenizer
    symbols


start_parser = fn ctx:
  advance ctx

